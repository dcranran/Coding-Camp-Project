# -*- coding: utf-8 -*-
"""notebook_recommendation system.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_2r0-YGSyZGhhHCTYtCjHBEbbEp2jxsK

# **Rekomendasi Film**

Proyek ini bertujuan untuk mengembangkan sistem rekomendasi film yang efektif, dengan memanfaatkan pendekatan yang relevan, untuk memberikan solusi nyata terhadap tantangan *information overload* dan meningkatkan pengalaman pengguna dalam menemukan film yang sesuai.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

from sklearn.model_selection import train_test_split

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

from tabulate import tabulate

from sklearn.metrics import mean_squared_error, mean_absolute_error
import random

"""## **Data Understanding**

Data yang digunakan dalam proyek sistem rekomendasi ini adalah **Movies & Ratings for Recommendation System**. Dataset ini bersumber dari platform Kaggle yang dapat diakses pada tautan [Movies & Ratings for Recommendation System](https://www.kaggle.com/datasets/nicoletacilibiu/movies-and-ratings-for-recommendation-system). Dataset terdiri dari dua *file* yaitu `movies.csv` dan `ratings.csv`. Dataset ini dirancang untuk membangun sistem rekomendasi film.
"""

movies = pd.read_csv('movies.csv')
ratings = pd.read_csv('ratings.csv')

"""### **Movies**"""

movies.head()

movies.shape

movies.info()

"""Data terdiri dari 9742 film dengan 3 fitur/kolom."""

movies.describe(include='object')

movies['title'].nunique()

duplicate_titles = movies[movies.duplicated(subset=['title'], keep=False)].sort_values('title')
duplicate_titles

"""Terdapat 5 judul film yang terduplikat karena perbedaan pada kolom `genres`, sehingga `movieId` menjadi berbeda."""

movies.isnull().sum()

"""Data tidak mengandung *missing value*.

### **Ratings**
"""

ratings.head()

ratings.shape

ratings.info()

"""Data ini terdiri dari 100.836 rating dengan 4 fitur/kolom."""

ratings.describe()

ratings.isnull().sum()

"""Data tidak mengandung *missing value*."""

ratings.duplicated().sum()

"""Data tidak mengandung nilai duplikat."""

plt.figure(figsize=(8, 6))
sns.countplot(data=ratings, x='rating', hue='rating', palette='Set2')
plt.title('Bar Chart of Ratings')
plt.xlabel('Rating')
plt.ylabel('Count')
plt.show()

"""Rating berkisar antara 0,5 hingga 5. dengan peningkatan 0,5. Semakin tinggi nilai rating menandakan user semakin menyukai film, dengan rating yang paling banyak diberikan oleh user adalah 4.

## **Data Preparation**

### **Data untuk Content-Based Filtering**

Data `movies.csv` disalin ke DataFrame baru yang dinamakan `df_content` untuk memudahkan pengenalan pada analisis selanjutnya.
"""

df_content = movies.copy()

df_content.head()

"""**Menghapus Film Duplikat**

Salah satu dari judul film yang terduplikat dihapus dengan mempertahankan baris judul film dengan genre lebih banyak karena perbedaan terdapat pada kolom `genres`.
"""

duplicate_titles = movies[movies.duplicated(subset=['title'], keep=False)].sort_values('title')
duplicate_titles

def count_genres(genres_str):
    return len(genres_str.split('|'))

duplicate_titles['genre_count'] = duplicate_titles['genres'].apply(count_genres)

idx_to_keep = (
    duplicate_titles
    .sort_values('genre_count', ascending=False)
    .drop_duplicates(subset='title', keep='first')
    .index
)

idx_to_drop = set(duplicate_titles.index) - set(idx_to_keep)
movieIds_to_drop = duplicate_titles.loc[list(idx_to_drop), 'movieId']

movies = movies[~movies['movieId'].isin(movieIds_to_drop)]
df_content = df_content[~df_content['movieId'].isin(movieIds_to_drop)]

"""**Mengubah `genre` Menjadi String Terpisah**

Kolom `genres` yang awalnya berisi string genre yang dipisahkan oleh tanda (|) diubah menjadi format string tunggal di mana setiap genre dipisahkan oleh spasi, kemudian kolom tersebut dinamakan `genre`.
"""

df_content['genres_list'] = df_content['genres'].apply(lambda x: x.split('|'))

df_content['genre'] = df_content['genres_list'].apply(lambda x: ' '.join(x))

df_content.drop(columns=['genres', 'genres_list'], inplace=True)

df_content.head()

"""**Melihat Genre Film**"""

unique_genres = set()
for genres in df_content['genre']:
    unique_genres.update(genres.split())
print(f"Jumlah genre unik: {len(unique_genres)}")
print(f"Daftar genre unik: {sorted(unique_genres)}")

"""Setelah dilakukan pemrosesan, diperoleh 22 genre unik (termasuk '(no genres listed)')

**TF-IDF Vectorizer**

Kolom `genre` dari `df_content` divektorisasi menggunakan TfidfVectorizer yang mengubah teks genre menjadi representasi numerik dalam bentuk matriks TF-IDF.
"""

tfidf = TfidfVectorizer()
tfidf.fit(df_content['genre'])
tfidf.get_feature_names_out()

tfidf_matrix = tfidf.fit_transform(df_content['genre'])
tfidf_matrix.shape

"""Diperoleh matriks berukuran (9737, 24)"""

tfidf_matrix.todense()

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tfidf.get_feature_names_out(),
    index=df_content.title
).sample(22, axis=1, replace=True).sample(10, axis=0)

"""### **Data untuk Collaborative Filtering**

Data `ratings.csv` disalin ke DataFrame baru yang dinamakan `df_collab` untuk memudahkan pengenalan pada analisis selanjutnya.
"""

df_collab = ratings.copy()

df_collab.head()

"""**Menghapus Kolom `timestamp`**

Kolom `timestamp` dihapus dari `df_collab` karena tidak akan digunakan dalam model.
"""

df_collab.drop(columns=['timestamp'], inplace=True)

"""**Encoding UserID**

Dilakukan encoding `userId` dan `movieId` menjadi indeks numerik sekuensial (dimulai dari 0) agar dapat digunakan sebagai input pada lapisan Embedding di model.

Dibuat daftar unik.
"""

user_id = df_collab['userId'].unique().tolist()
print('list userID: ', user_id)

"""Dilakukan pemetaan (*mapping*) dari `userId` asli ke representasi numerik yang dimulai dari 0 (`user_to_user_encoded`), dan sebaliknya (`user_encoded_to_user`)."""

user_to_user_encoded = {x: i for i, x in enumerate(user_id)}
print('encoded userID : ', user_to_user_encoded)

user_encoded_to_user = {i: x for i, x in enumerate(user_id)}
print('encoded angka ke userID: ', user_encoded_to_user)

"""**Encoding MovieID**

Dibuat daftar unik.
"""

movie_id = df_collab['movieId'].unique().tolist()
print('list movieID: ', movie_id)

"""Dilakukan pemetaan (*mapping*) dari `movieId` asli ke representasi numerik yang dimulai dari 0 (`movie_to_movie_encoded`), dan sebaliknya (`movie_encoded_to_movie`)."""

movie_to_movie_encoded = {x: i for i, x in enumerate(movie_id)}
print('encoded movieID : ', movie_to_movie_encoded)

movie_encoded_to_movie = {i: x for i, x in enumerate(movie_id)}
print('encoded angka ke movieID: ', movie_encoded_to_movie)

"""**Mapping UserID & MovieID**

Nilai `userId` dan `movieId` pada `df_collab` diperbarui dengan nilai hasil encoding tersebut, disimpan dalam kolom baru `user` dan `movie`.
"""

df_collab['user'] = df_collab['userId'].map(user_to_user_encoded)
df_collab['movie'] = df_collab['movieId'].map(movie_to_movie_encoded)

"""**Menghitung Jumlah User, Movie, dan Rating**"""

num_users = len(user_to_user_encoded)
print('Jumlah User: ', num_users)

num_movies = len(movie_to_movie_encoded)
print('Jumlah Movie: ', num_movies)

min_rating = min(df_collab['rating'])
max_rating = max(df_collab['rating'])
print('Rating Minimum: ', min_rating)
print('Rating Maksimum: ', max_rating)

"""**Menghitung Jumlah Rating untuk Tiap Film**"""

df_collab.groupby('movieId').size().reset_index(name='count').sort_values(by='count', ascending=False)

df_collab.groupby('movieId').size().reset_index(name='count').sort_values(by='count', ascending=False).head(10)

"""**Menghitung Jumlah Rating yang Diberikan Setiap User**"""

df_collab.groupby('userId').size().reset_index(name='count').sort_values(by='count', ascending=False)

df_collab.groupby('userId').size().reset_index(name='count').sort_values(by='count', ascending=False).head(10)

"""**Split Data**"""

X = df_collab[['user', 'movie']].values
y = df_collab['rating']

"""**Scaling Rating**

Normalisasi rating ke rentang 0-1 (normalisasi min-max) karena model akan menggunakan fungsi aktivasi sigmoid pada lapisan output, yang menghasilkan nilai antara 0 dan 1.
"""

y = y.apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

"""**Membagi Data Training dan Validation**

Data `df_collab` (fitur `user` dan `movie` sebagai X, dan `rating` yang sudah dinormalisasi sebagai y) dibagi menjadi data latih (X_train, y_train) dan data validasi (X_val, y_val) dengan proporsi 80:20.
"""

X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, random_state=42, shuffle=True
)

X_train.shape, X_val.shape, y_train.shape, y_val.shape

"""## **Modeling**

### **Content-Based Filtering**

**Cosine Similarity**

Dihitung *cosine similarity* antar semua pasangan film berdasarkan matriks TF-IDF mereka.
"""

cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

cosine_sim_df = pd.DataFrame(cosine_sim, index=df_content['title'], columns=df_content['title'])
print('Shape:', cosine_sim_df.shape)

cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""Proses ini menghasilkan matriks kemiripan (9737 x 9737) di mana setiap sel (i,j) berisi skor kemiripan antara film i dan film j.

**Recommendation**

Dibuat fungsi yang menerima judul film sebagai input dan mengembalikan Top-N (default N=5) film yang paling mirip, tidak termasuk film input itu sendiri. Fungsi ini mencari film dengan skor kemiripan kosinus tertinggi terhadap film input.
"""

def movie_recommendations(title, similarity_data=cosine_sim_df, items=df_content[['movieId', 'title', 'genre']], k=5):
    index = similarity_data.loc[:, title].to_numpy().argpartition(range(-1, -k, -1))

    closest = similarity_data.columns[index[-1:-(k+2):-1]]
    closest = closest.drop(title, errors='ignore')

    result = pd.DataFrame({'title': closest})
    return result.merge(items, on='title', how='left').head(k)

df_content[df_content.title.eq('Red Dragon (2002)')]

cb_recommend = movie_recommendations('Red Dragon (2002)')
cb_recommend

"""### **Collaborative Filtering**

#### **Training**

Dibangun model *neural network* menggunakan TensorFlow Keras dan dinamakan RecommenderNet
"""

class RecommenderNet(tf.keras.Model):

  def __init__(self, num_users, num_movies, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_movies = num_movies
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding(
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1)
    self.movie_embedding = layers.Embedding(
        num_movies,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.movie_bias = layers.Embedding(num_movies, 1)

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0])
    user_bias = self.user_bias(inputs[:, 0])
    movie_vector = self.movie_embedding(inputs[:, 1])
    movie_bias = self.movie_bias(inputs[:, 1])

    dot_user_movie = tf.tensordot(user_vector, movie_vector, 2)

    x = dot_user_movie + user_bias + movie_bias

    return tf.nn.sigmoid(x)

"""Model dikompilasi dengan loss function BinaryCrossentropy (karena output sigmoid mirip probabilitas), optimizer Adam dengan learning rate 0.001, dan metrik RootMeanSquaredError."""

model = RecommenderNet(num_users, num_movies, 50)

model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""Digunakan callback EarlyStopping untuk menghentikan training jika val_root_mean_squared_error tidak membaik selama 3 epoch, dan mengembalikan bobot terbaik."""

early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_root_mean_squared_error',
    patience=3,
    restore_best_weights=True
)

"""Model dilatih menggunakan data latih (X_train, y_train) dengan ukuran batch 8, selama maksimal 100 epoch, dan divalidasi menggunakan data validasi (X_val, y_val)."""

history = model.fit(
    x = X_train,
    y = y_train,
    batch_size = 8,
    epochs = 100,
    validation_data = (X_val, y_val),
    callbacks=[early_stopping]
)

"""Training berhenti pada epoch ke-12 karena *early stopping*."""

last_epoch = len(history.history['loss']) - 1
print(f"Epoch terakhir: {last_epoch + 1}")
print(f"Loss: {history.history['loss'][last_epoch]}")
print(f"Root Mean Squared Error (Train): {history.history['root_mean_squared_error'][last_epoch]}")
print(f"Val Loss: {history.history['val_loss'][last_epoch]}")
print(f"Root Mean Squared Error (Val): {history.history['val_root_mean_squared_error'][last_epoch]}")

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

"""Hasil *modeling* menunjukkan bahwa model berhasil mempelajari pola dari data latih, sebagaimana tercermin dari penurunan kurva Train RMSE yang konsisten pada plot metrik. Meskipun demikian, kurva Val RMSE mengalami penurunan hanya pada epoch-epoch awal (sekitar epoch 0-2) sebelum akhirnya mendatar di sekitar nilai 0,1999. Perilaku Val RMSE yang mendatar ini, ditambah dengan adanya kesenjangan antara nilai akhir Train RMSE (0,1815) yang lebih rendah dibandingkan Val RMSE (0,1989), serta Train Loss (0,5911) yang lebih rendah dari Val Loss (0,6057), mengindikasikan terjadinya gejala *overfitting*.

**Recommendation**

Diambil satu userId secara acak kemudian diidentifikasi film-film yang sudah ditonton oleh pengguna tersebut kemudian diambil daftar film yang belum ditonton oleh pengguna tersebut.
"""

user_id = df_collab.userId.sample(1).iloc[0]
movie_watched_by_user = df_collab[df_collab.userId == user_id]

movie_not_watched = movies[~movies['movieId'].isin(movie_watched_by_user.movieId.values)]['movieId']
movie_not_watched = list(
    set(movie_not_watched)
    .intersection(set(movie_to_movie_encoded.keys()))
)

movie_not_watched = [[movie_to_movie_encoded.get(x)] for x in movie_not_watched]
user_encoder = user_to_user_encoded.get(user_id)
user_movie_array = np.hstack(
    ([[user_encoder]] * len(movie_not_watched), movie_not_watched)
)

"""Model RecommenderNet digunakan untuk memprediksi rating yang mungkin diberikan oleh pengguna tersebut terhadap film-film yang belum ditonton kemudian merekomendasikan 10 film dengan prediksi rating tertinggi kepada pengguna."""

print(f"\nRekomendasi untuk User ID: {user_id}")
print("=" * 40)
print("Film Favorit Pengguna (Rating Tertinggi)")
print("-" * 40)

top_movie_user = movie_watched_by_user.sort_values(by='rating', ascending=False).head(5).movieId.values
favorite_movies = movies[movies['movieId'].isin(top_movie_user)][['title', 'genres']]
print(tabulate(favorite_movies.values, headers=["Title", "Genres"], tablefmt="fancy_grid"))

print("\nTop 10 Rekomendasi Film untuk Pengguna Ini")
print("-" * 40)

ratings = model.predict(user_movie_array).flatten()
recommended_movie_ids = [movie_encoded_to_movie.get(movie_not_watched[x][0]) for x in ratings.argsort()[-10:][::-1]]
recommended_movies = movies[movies['movieId'].isin(recommended_movie_ids)][['title', 'genres']]
print(tabulate(recommended_movies.values, headers=["Title", "Genres"], tablefmt="fancy_grid"))

"""## **Evaluation**

### **Content-Based Filtering**

Pertama, dilakukan penggabungan data `df_collab` dan `df_content` untuk memperoleh data yang lengkap.
"""

df = pd.merge(df_collab, df_content, on='movieId')

"""Dibangun fungsi untuk menghasilkan rekomendasi berbasis konten untuk sekelompok user."""

def get_cb_recommendations_for_users(users, k=10):
    cb_recommendations = {}
    for user in users:
        # Film yang sudah ditonton
        watched_ids = set(df[df['userId'] == user]['movieId'])

        # Film yang disukai user
        liked_titles = set(df[(df['userId'] == user) & (df['rating'] >= 4)]['title'])

        rec_movie_ids = []
        for title in liked_titles:
            recs = movie_recommendations(title, k=k + len(watched_ids))
            rec_ids = [mid for mid in recs['movieId'] if mid not in watched_ids]
            rec_movie_ids.extend(rec_ids)

        cb_recommendations[user] = list(dict.fromkeys(rec_movie_ids))[:k]
    return cb_recommendations

"""Sebanyak 50 user dipilih secara acak dari dataset untuk dievaluasi. Rekomendasi berbasis konten (top-10) dihasilkan untuk ke-50 pengguna sampel, dan untuk setiap pengguna sampel, dibuat "ground truth" yaitu daftar movieId dari film-film yang benar-benar mereka sukai (diberi rating 4 atau lebih tinggi)."""

sample_users = random.sample(list(df['userId'].unique()), 50)

cb_recommendations = get_cb_recommendations_for_users(sample_users, k=10)

ground_truth = {
    user: df[(df['userId'] == user) & (df['rating'] >= 4)]['movieId'].tolist()
    for user in sample_users
}

"""Dihitung precision@k dan recall@k berdasarkan daftar film yang direkomendasikan (`recommended_ids`) dan daftar film yang relevan/disukai user (`relevant_ids`)."""

def precision_recall_at_k(recommendations, ground_truth, k):
    precisions = []
    recalls = []
    for user in recommendations:
        recommended_ids = set(recommendations[user][:k])
        relevant_ids = set(ground_truth.get(user, []))
        if not relevant_ids:
            continue
        true_positives = len(recommended_ids & relevant_ids)
        precision = true_positives / k
        recall = true_positives / len(relevant_ids)
        precisions.append(precision)
        recalls.append(recall)
    avg_precision = sum(precisions) / len(precisions) if precisions else 0
    avg_recall = sum(recalls) / len(recalls) if recalls else 0
    return avg_precision, avg_recall

precision, recall = precision_recall_at_k(cb_recommendations, ground_truth, k=10)
print(f'Precision@10: {precision:.4f}')
print(f'Recall@10: {recall:.4f}')

"""Hasil 0,0 untuk Precision@10 dan Recall@10 menunjukkan bahwa model Content-Based Filtering yang diimplementasikan, dengan fokus utama pada genre sebagai fitur penentu, tidak berhasil menghasilkan rekomendasi yang relevan untuk pengguna dalam konteks evaluasi yang dilakukan. Pengguna tidak menemukan film yang mereka sukai dalam 10 rekomendasi teratas yang diberikan oleh sistem ini.

### **Collaborative Filtering**
"""

y_pred = model.predict(X_val)
y_pred_rescaled = y_pred * (max_rating - min_rating) + min_rating
y_val_rescaled = y_val * (max_rating - min_rating) + min_rating

rmse = np.sqrt(mean_squared_error(y_val_rescaled, y_pred_rescaled))
mae = mean_absolute_error(y_val_rescaled, y_pred_rescaled)

print(f"RMSE: {rmse:.4f}")
print(f"MAE: {mae:.4f}")

"""Secara keseluruhan, model *Collaborative Filtering* menunjukkan kemampuan yang moderat hingga baik dalam memprediksi preferensi rating pengguna. Dengan MAE sekitar 0,73, model ini rata-rata dapat memprediksi rating dengan kesalahan kurang dari satu poin pada skala rating yang ada. Perbedaan antara nilai RMSE dan MAE (RMSE > MAE) menyiratkan bahwa ada beberapa prediksi yang memiliki kesalahan yang relatif besar, yang lebih signifikan dipenalti oleh RMSE. Meskipun demikian, performa ini dianggap cukup baik dan menunjukkan bahwa model telah berhasil menangkap sebagian besar pola preferensi pengguna dari data rating historis.  """