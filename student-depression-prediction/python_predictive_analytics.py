# -*- coding: utf-8 -*-
"""predictive_analytics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17YXfJzG3w0gmF4py4fpKtTrvzH49oFWW

# **Analisis Prediksi Kesehatan Mental Siswa**

Proyek ini bertujuan untuk mengembangkan model prediksi depresi pada siswa dengan memanfaatkan dataset yang ada (Student Depression Dataset) dan teknik machine learning. Dengan menggunakan dataset yang tersedia dan teknik machine learning, proyek ini berpotensi memberikan kontribusi nyata dalam upaya meningkatkan kesejahteraan mental siswa di lingkungan pendidikan.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

from sklearn.preprocessing import OrdinalEncoder, LabelEncoder, OneHotEncoder
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler

from sklearn.model_selection import train_test_split, cross_val_score
from imblearn.combine import SMOTETomek
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, balanced_accuracy_score, roc_auc_score, classification_report, confusion_matrix
from sklearn.model_selection import cross_val_score

from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import HistGradientBoostingClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier

from sklearn.model_selection import RandomizedSearchCV
import shap

"""## **Data Understanding**

Data yang digunakan dalam proyek analisis prediksi kesehatan mental (depresi) mahasiswa ini adalah **"Student Depression Dataset"**. Dataset ini bersumber dari platform Kaggle yang dapat diakses pada tautan [Student Depression Dataset](https://www.kaggle.com/datasets/adilshamim8/student-depression-dataset/data).Tujuan utama dataset ini adalah untuk menyediakan data bagi penelitian dan pengembangan model yang dapat mengidentifikasi faktor-faktor terkait depresi di kalangan siswa.
"""

df = pd.read_csv("student_depression_dataset.csv")

df.head()

"""Akan dilakukan tahapan Data Understanding untuk memahami dataset yang akan digunakan."""

df.shape

df.info()

"""Dataset ini berisi 27.901 data record siswa dengan 18 fitur yang mencakup informasi demografis, akademik, dan indikator kesehatan mental."""

df.describe()

"""Tabel di atas menunjukkan statistik deskriptif dari variabel numerik. Secara keseluruhan, dapat diketahui bahwa umur siswa berada dalam rentang 18-59 tahun dengan rata-rata 25 tahun. `Academic Pressure`, `Work  Pressure`, `Study Satisfaction`, dan `Job Satisfaction` menggunakan skala rating. IPK siswa berada dalam rentang 0-10 dengan rata-rata 7,65. Jam kerja/belajar siswa berada dalam rentang 0-12 jam dengan rata-rata 7,15 jam. Variabel terget yaitu `Depression` seharusnya merupakan variabel biner."""

df.describe(include='object')

"""Dari tabel di atas, dapat diketahui jumlah kategori, nilai modus, dan jumlah kemunculan modus dari setiap variabel kategorik"""

df.isnull().sum()

"""Data tidak mengandung *missing value*."""

df.duplicated().sum()

"""Tidak terdapat data terduplikat."""

num_features = df.select_dtypes(include=['number']).columns.tolist()
cat_features = df.select_dtypes(include=['object', 'category']).columns.tolist()

print("Fitur Numerik:", num_features)
print("Fitur Kategorikal:", cat_features)

"""Data awal terdiri dari 9 variabel kategorik dan 9 variabel numerik."""

for var in cat_features:
    print(df[var].value_counts())
    print('\n')

"""Output di atas menunjukkan sebaran tiap kategori pada variabel kategorik. Kita juga dapat mengetahui kategori apa saja yang terdapat pada suatu variabel dan mengidentifikasi apabila terdapat nilai yang tidak valid, misalnya kategori ? pada variabel `Financial Stress`.

**Histogram Variabel Numerik**
"""

fig, axes = plt.subplots(3, 3, figsize=(15, 12))
axes = axes.flatten()

num = [col for col in num_features if col not in ['id', 'Depression']]

for i, var in enumerate(num):
    q1 = df[var].quantile(0.25)
    median = df[var].median()
    q3 = df[var].quantile(0.75)
    mean = df[var].mean()

    sns.histplot(data=df, x=var, bins=20, kde=True, ax=axes[i])

    axes[i].axvline(q1, color='red', linestyle='--', label='Q1')
    axes[i].axvline(median, color='green', linestyle='--', label='Median')
    axes[i].axvline(q3, color='blue', linestyle='--', label='Q3')
    axes[i].axvline(mean, color='purple', linestyle='--', label='Mean')

    axes[i].set_title(f'Histogram of {var}')
    axes[i].set_xlabel(var)
    axes[i].set_ylabel('Frequency')
    axes[i].legend()

for j in range(len(num), len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

"""Secara keseluruhan, histogram-histogram di atas memberikan gambaran mengenai distribusi berbagai variabel numerik dalam dataset. Garis-garis vertikal menunjukkan Kuartil 1 (Q1), Median (Q2), Kuartil 3 (Q3), dan Rata-rata (Mean). Dari histogram dapat diketahui bahwa dataset mencakup siswa dengan rentang usia 18-30 tahun. Banyak variabel seperti tekanan akademik dan kepuasan studi menggunakan skala rating.Faktor pekerjaan (tekanan dan kepuasan) tidak relevan bagi sebagian besar sampel, karena mayoritas siswa tidak bekerja. Kemudian, IPK dan jam belajar menunjukkan pola yang bervariasi di antara siswa.

**Bar Chart Variabel Kategorik**
"""

fig, axes = plt.subplots(3, 3, figsize=(15, 12))
axes = axes.flatten()

for i, var in enumerate(cat_features):
  sns.countplot(data=df, x=var, ax=axes[i], hue=var, palette='Set2')
  axes[i].set_title(f'Bar Chart of {var}')
  axes[i].set_xlabel('Class')
  axes[i].set_ylabel('Count')
  axes[i].legend()
  axes[i].tick_params(axis='x', rotation=45)

for j in range(len(cat_features), len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

"""Secara keseluruhan, bar chart di atas menggambarkan distribusi frekuensi dari berbagai variabel kategori dalam dataset. Dari bar chart dapat diketahui bahwa dataset didominasi oleh siswa kelas 12 dengan distribusi gender yang cukup seimbang dan berasal dari berbagai kota. Banyak siswa mengalami kekurangan tidur, kebiasaan makan yang kurang baik, stres finansial, dan yang paling mencolok, tingginya prevalensi ideasi bunuh diri serta riwayat keluarga dengan penyakit mental."""

plt.figure(figsize=(8, 6))
sns.countplot(data=df, x='Depression', hue='Depression', palette='Set2')
plt.title('Bar Chart of Target Variable')
plt.xlabel('Class')
plt.ylabel('Count')
plt.show()

"""Dari bar chart di atas, terlihat bahwa terjadi ketidakseimbangan antara kelas 0 (siswa tidak mengalami depresi) dan kelas 1 (siswa mengalami depresi). Oleh karena itu, perlu dilakukan pre-processing supaya hasil klasifikasi tidak bias pada salah satu kelas.

**Korelasi Antar Variabel Numerik**
"""

corr_matrix = df[num].corr()
plt.figure(figsize=(8, 6))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Heatmap of Correlation Matrix')
plt.show()

"""Berdasarkan heatmap di atas, terlihat tidak ada hubungan linear yang kuat antara sebagian besar  variabel, kecuali antara `Work Pressure` dan `Job Satisfaction`. Hal ini menunjukkan bahwa untuk memahami hubungan yang lebih kompleks (non-linear) mungkin diperlukan teknik analisis yang lebih canggih atau pertimbangan interaksi antar variabel, karena korelasi linear sederhana tidak banyak memberikan informasi.

## **Data Preparation**

### **Data Cleaning**

Tahap ini melibatkan penanganan  masalah dalam data mentah dan dilakukan untuk menjamin kualitas data.

Dilakukan penghapusan kolom `ID` karena variabel ini tidak memberikan informasi yang bermanfaat dalam prediksi.
"""

df.drop(columns=['id'], inplace=True)

"""Dilakukan penghapusan baris yang memiliki nilai '?' pada kolom `Financial Stress`."""

df = df[df['Financial Stress'] != '?']

"""### **Feature Encoding**

Dibuat salinan dataframe agar proses encoding tidak mengubah dataframe asli.
"""

df_encoded = df.copy()

"""Encoding dibagi menjadi empat tahap.

**Ordinal Encoding**

Diterapkan pada fitur ordinal yang memiliki urutan jelas, yaitu `Sleep Duration`, `Dietary Habits`, dan `Degree`.
"""

# Ordinal Encoding
sleep_order = ['Others', 'Less than 5 hours', '5-6 hours', '7-8 hours', 'More than 8 hours']
diet_order = ['Others', 'Unhealthy', 'Moderate', 'Healthy']
degree_order = [
    'Others', 'Class 12', 'B.Ed', 'B.Com', 'BA', 'BBA', 'BCA', 'BSc', 'B.Tech', 'BE', 'BHM', 'B.Arch', 'B.Pharm', 'LLB', 'MBBS',
    'M.Com', 'MBA', 'MA', 'MCA', 'MSc', 'M.Ed', 'M.Tech', 'ME', 'M.Pharm', 'MD', 'PhD', 'LLM', 'MHM',
]

ordinal_cols = ['Sleep Duration', 'Dietary Habits', 'Degree']
ordinal_categories = [sleep_order, diet_order, degree_order]

ord_enc = OrdinalEncoder(categories=ordinal_categories, handle_unknown='use_encoded_value', unknown_value=-1)
df_encoded[ordinal_cols] = ord_enc.fit_transform(df_encoded[ordinal_cols])

"""**One-hot Encoding**

Diterapkan pada fitur nominal yang tidak memiliki urutan, yaitu `Profession`. Profesi yang jarang (<10 data dengan profesi tersebut) akan digabungkan menjadi bernilai 'Others'.
"""

# One-hot Encoding
prof_counts = df_encoded['Profession'].value_counts()
rare_profs = prof_counts[prof_counts < 10].index
df_encoded['Profession'] = df_encoded['Profession'].replace(rare_profs, 'Others')

ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
prof_ohe = ohe.fit_transform(df_encoded[['Profession']])
prof_feature_names = ohe.get_feature_names_out(['Profession'])
df_prof = pd.DataFrame(prof_ohe, columns=prof_feature_names, index=df_encoded.index)
df_encoded = pd.concat([df_encoded.drop(columns=['Profession']), df_prof], axis=1)

"""**Label Encoding**

Diterapkan pada fitur ordinal yang memiliki dua nilai (biner), yaitu `Gender`, `Have you ever had suicidal thoughts ?`, dan `Family History of Mental Illness`.
"""

# Label Encoding
le = LabelEncoder()

df_encoded['Gender'] = le.fit_transform(df_encoded['Gender'])
df_encoded['Have you ever had suicidal thoughts ?'] = le.fit_transform(df_encoded['Have you ever had suicidal thoughts ?'])
df_encoded['Family History of Mental Illness'] = le.fit_transform(df_encoded['Family History of Mental Illness'])

"""**Frequency Encoding**

Diterapkan pada fitur dengan kardinalitas tinggi, yaitu `City`.
"""

# Frequency Encoding
city_counts = df_encoded['City'].value_counts()
df_encoded['City'] = df_encoded['City'].map(city_counts)
df_encoded['City'] = df_encoded['City'].fillna(0)

"""Fitur `Financial Stress` yang awalnya sudah dalam nilai angka hanya perlu diubah tipe datanya menjadi numerik."""

df_encoded['Financial Stress'] = pd.to_numeric(df_encoded['Financial Stress'], errors='coerce')

df_encoded.info()

"""Seluruh variabel telah bernilai numerik dan dapat diproses oleh algoritma *machine learning*.

### **Feature Selection**

Tahap ini bertujuan untuk memilih subset fitur yang paling relevan dan informatif untuk memprediksi depresi. Digunakan metode filter berbasis uji statistik, yaitu ANOVA F-test, untuk memiliki 10 fitur teratas yang memiliki hubungan statistik terkuat dengan variabel target.
"""

X = df_encoded.drop(columns=['Depression'])
y = df_encoded['Depression']

selector = SelectKBest(score_func=f_classif, k=10)
X_new = selector.fit_transform(X, y)

selected_features = X.columns[selector.get_support()]
selected_features.tolist()

"""Terpilih 10 fitur teratas yang memiliki hubungan statistik terkuat dengan variabel target."""

df_selected = df_encoded[selected_features.tolist() + ['Depression']]
df_selected.head()

"""### **Data Splitting**

Data dibagi menjadi data train (80%) dan data test (20%). Data train digunakan untuk melatih model, sementara data test digunakan untuk menguji kemampuan model pada data yang belum pernah dilihat.
"""

X = df_selected.drop(columns=['Depression'])
y = df_selected['Depression']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Training set size: {X_train.shape[0]}")
print(f"Test set size: {X_test.shape[0]}")

"""*Splitting data* dilakukan sebelum scaling untuk menghindari *data leakage*

### **Feature Scaling**

Tahap ini bertujuan untuk menyeragamkan rentang nilai fitur-fitur numerik. Metode yang akan digunakan adalah StandardScaler, yang mengubah data sehingga memiliki rata-rata 0 dan standar deviasi 1.
"""

num_features_update = ['Age', 'CGPA', 'Work/Study Hours']

scaler = StandardScaler()
X_train_scaled = X_train.copy()
X_test_scaled = X_test.copy()

X_train_scaled[num_features_update] = scaler.fit_transform(X_train[num_features_update])
X_test_scaled[num_features_update] = scaler.transform(X_test[num_features_update])

"""### **Imbalance Handling**

Dari proses EDA diketahui bahwa distribusi kelas pada variabel target tidak seimbang. Jika dataset tidak seimbang, model cenderung menjadi bias terhadap kelas mayoritas dan memiliki performa yang buruk dalam memprediksi kelas minoritas. Oleh karena itu, akan dilakukan penanganan dengan metode SMOTETomek yang merupakan teknik *resampling* gabungan antara SMOTE (Synthetic Minority Over-sampling Technique) dan Tomek Links.
"""

smote = SMOTETomek(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)

print("Distribusi kelas sebelum penanganan:")
print(y_train.value_counts())
print("\nDistribusi kelas setelah penanganan:")
print(pd.Series(y_train_smote).value_counts())

"""Diperoleh hasil distribusi kelas yang seimbang antara kelas 0 (siswa tidak mengalami depresi) dan kelas 1 (siswa mengalami depresi).

## **Modeling**

Sesuai dengan *goals* yang telah ditetapkan, tahapan ini melibatkan eksperimen dengan berbagai algoritma dan penyetelan parameter untuk mendapatkan performa terbaik. Berdasarkan *Solution Statements* yang mengusulkan penggunaan dua atau lebih algoritma, pendekatan yang dilakukan adalah dengan menguji berbagai model dan kemudian memilih yang terbaik untuk dioptimalkan lebih lanjut.
"""

def all_model(list_model, X_train, X_test, y_train, y_test):

    result = []
    for model in list_model:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        recall = recall_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        bal_accuracy = balanced_accuracy_score(y_test, y_pred)
        accuracy = accuracy_score(y_test, y_pred)
        auc = roc_auc_score(y_test, y_pred)

        hasil = {
            'Model' : type(model).__name__,
            'Accuracy' : accuracy,
            'Recall' : recall,
            'Precision' : precision,
            'F1 Score' : f1,
            'AUC' : auc,
        }

        result.append(hasil)

    result_all = pd.DataFrame(result)

    return result_all

""" Langkah pertama adalah menguji serangkaian algoritma *machine learning* yang beragam untuk mendapatkan gambaran performa *baseline* masing-masing."""

daftar_model = [
    SVC(),
    KNeighborsClassifier(),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    LogisticRegression(max_iter = 1000, random_state=0),
    GaussianNB(),
    QuadraticDiscriminantAnalysis(),
    AdaBoostClassifier(random_state=0),
    GradientBoostingClassifier(random_state=0),
    XGBClassifier(random_state=0),
    LGBMClassifier(random_state=0, verbose=-1),
    CatBoostClassifier(random_state=0),
    HistGradientBoostingClassifier(random_state=0)
]

"""Dilakukan iterasi pada setiap model dalam daftar, kemudian performa model diukut menggunakan metrik evaluasi yang terdiri dari *accuracy*, *precision*, *recall*, *F1-score*, dan AUC-ROC."""

model_klasifikasi = all_model(daftar_model, X_train_smote, X_test_scaled, y_train_smote, y_test)
model_klasifikasi

"""Setelah diperoleh hasil performa dari seluruh model, akan dipilih model yang terbaik."""

model_klasifikasi.sort_values(by='Accuracy', ascending=False, inplace=True)
model_klasifikasi

"""Dipilih algoritma Logistic Regression sebagai model terbaik karena menghasilkan nilai akurasi terbaik, dengan nilai metrik lain yang juga bersaing dengan algoritma lain. Selain itu, model Logistic Regression lebih sederhana dan mudah diinterpretasi."""

logistic_model = LogisticRegression(max_iter=1000)
logistic_model.fit(X_train_smote, y_train_smote)

"""Setelah terpilih model terbaik,  langkah selanjutnya adalah melakukan *improvement* dengan *hyperparameter tuning* untuk mencari kombinasi parameter terbaik yang memberikan performa maksimal. Digunakan RandomizedSearchCV untuk mencari kombinasi parameter terbaik dari model regresi logistik."""

param_dist = {
    'C': np.logspace(-3, 3, 10),
    'solver': ['liblinear', 'lbfgs'],
    'penalty': ['l2'],
    'max_iter': [100, 200, 500, 1000]
}

logreg = LogisticRegression(random_state=0)
random_search = RandomizedSearchCV(
    logreg,
    param_distributions=param_dist,
    n_iter=20,
    scoring='accuracy',
    cv=5,
    random_state=42,
    n_jobs=-1,
    verbose=2
)
random_search.fit(X_train_smote, y_train_smote)

print("Best Params:", random_search.best_params_)
print("Best Accuracy:", random_search.best_score_)

"""Selanjutnya akan dilakukan evaluasi pada model regresi logistik dengan parameter terbaik.

## **Evaluation**
"""

best_logreg = random_search.best_estimator_
y_pred_best = best_logreg.predict(X_test_scaled)

print("Classification Report:\n", classification_report(y_test, y_pred_best))

""" Berdasarkan *Classification Report*, model menunjukkan kinerja yang sangat baik, terutama kemampuannya untuk mengidentifikasi 87% kasus depresi aktual (Recall tinggi). Didukung dengan Precision yang baik, model ini mencapai keseimbangan yang efektif antara menemukan kasus dan menjaga keakuratan prediksi positif."""

cm_best = confusion_matrix(y_test, y_pred_best)
plt.figure(figsize=(6, 5))
sns.heatmap(cm_best, annot=True, fmt="d", cmap="Blues", xticklabels=set(y_test), yticklabels=set(y_test))
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix (Best Logistic Regression)")
plt.show()

""" Berdasarkan *Confusion Matrix* dapat diketahui bahwa:
   - Model dengan benar memprediksi 1896 mahasiswa sebagai Tidak Depresi (*True Negative*).
   - Model salah memprediksi 452 mahasiswa sebagai Depresi, padahal sebenarnya mereka Tidak Depresi (*False Positive*).
   - Model salah memprediksi 420 mahasiswa sebagai Tidak Depresi, padahal sebenarnya mereka Depresi (*False Negative*).
   - Model dengan benar memprediksi 2812 mahasiswa sebagai Depresi (*True Positive*).
"""

explainer = shap.Explainer(logistic_model, X_train_smote)
shap_values = explainer(X_test_scaled)

shap.summary_plot(shap_values, X_test_scaled, feature_names=X_test_scaled.columns)

"""SHAP Summary Plot mengidentifikasi faktor-faktor risiko paling signifikan untuk depresi pada siswa berdasarkan model yang telah dibangun. **Faktor-faktor utama adalah adanya pikiran bunuh diri, tingginya tekanan akademik, dan tingginya stres finansial.** Faktor-faktor lain seperti usia muda, kebiasaan makan buruk, jam belajar panjang, kepuasan belajar rendah, riwayat keluarga, dan IPK rendah juga berkontribusi secara signifikan."""